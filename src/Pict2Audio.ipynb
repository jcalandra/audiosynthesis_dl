{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pict2Audio",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcalandra/audiosynthesis_dl/blob/master/src/Pict2Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c2bDMFdrIp9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pict2Audio : A Neural Network that associates Pictures to Audio Descriptors\n",
        "\n",
        "This project consists in associating a sound with one or more characteristics defined by audio descriptors with a picture drawn by a composer . The long-term goal is to allow the composer to develop his own composition language in order to associate it with sounds from some effects banks.\n",
        "\n",
        "I will first be interested in pitch, and I will propose as input to a Convolutional Neural Network trained for classification a database of couples:\n",
        "- an image drawn by the composer,\n",
        "- the label of the sound extract corresponding to the associated note pitch.\n",
        "After training, we want to obtain a sound for a given image at the input of the network.\n",
        "\n",
        "Validation tests will be conducted by verifying that the sounds obtained correspond to the desired descriptors.\n",
        "\n",
        "## Importation of the libraries\n",
        "\n",
        "First, we need to import all the package and libraries necessary to run the code.\n",
        "\n",
        "The backend Tensorflow is used with the library Keras to implement the neural network."
      ]
    },
    {
      "metadata": {
        "id": "_ZFRXiCWomB3",
        "colab_type": "code",
        "outputId": "71e1ae15-0f30-4765-f70c-31ad66fb3220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from PIL import Image\n",
        "import os, sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from os import walk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cdsYKWFeH6kq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Importation of the Dataset :\n",
        "\n",
        "the dataset is imported from github, using the repository audiosynthesis_dl. In this repository, you can also find documentation about sound synthesis using Neural Networks."
      ]
    },
    {
      "metadata": {
        "id": "iK-IhlQAnK21",
        "colab_type": "code",
        "outputId": "640b5f57-3e53-4b35-8259-4aba5a6a342b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/jcalandra/audiosynthesis_dl.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'audiosynthesis_dl'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (249/249), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 588 (delta 6), reused 246 (delta 4), pack-reused 339\u001b[K\n",
            "Receiving objects: 100% (588/588), 9.70 MiB | 31.54 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KNeHidkTnaNi",
        "colab_type": "code",
        "outputId": "3a73dff1-e3d3-4e87-f558-0c0cc401c353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('tensorflow:', tf.__version__)\n",
        "print('keras:', keras.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow: 1.13.1\n",
            "keras: 2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SgWQoVexJylW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the datas\n"
      ]
    },
    {
      "metadata": {
        "id": "Kl1ulRj9J7GW",
        "colab_type": "code",
        "outputId": "9a43c3b1-c07b-4e38-f218-7bb9ff892a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "#Loading the pictures.\n",
        "x_train = np.empty((0,400,400,1))\n",
        "for imgP in os.listdir( \"./audiosynthesis_dl/data/pitch_img/img_train\")[:] :\n",
        "  if imgP.split(\".\")[-1] != \"git\":\n",
        "    img = image.load_img( \"./audiosynthesis_dl/data/pitch_img/img_train/\"+imgP, \n",
        "                             target_size=(400, 400),\n",
        "                             color_mode='grayscale')\n",
        "    # To input our values in our network Conv2D layer, we need to reshape the \n",
        "    # datasets, i.e., pass from (60, 400, 400) to (60, 400, 400, 1) where 1 is \n",
        "    # the number of channels of our images\n",
        "    x_train = np.concatenate((x_train,np.reshape(img,(1,400,400,1))),axis=0)\n",
        "    \n",
        "x_test = np.empty((0,400,400,1))\n",
        "for imgP in os.listdir( \"./audiosynthesis_dl/data/pitch_img/img_test\")[:] :\n",
        "  if imgP.split(\".\")[-1] != \"git\":\n",
        "    img = image.load_img( \"./audiosynthesis_dl/data/pitch_img/img_test/\"+imgP, \n",
        "                             target_size=(400, 400),\n",
        "                             color_mode='grayscale')\n",
        "    x_test = np.concatenate((x_test,np.reshape(img,(1,400,400,1))),axis=0)\n",
        "\n",
        "\n",
        "# x_train : 252 images of size 400x400, i.e., x_train.shape = (120, 400, 400)\n",
        "# y_train : 252 sounds-labels (from 0 to 11 corresponding to each pitch in 1 octave)\n",
        "# x_test  : 84 images of size 400x400, i.e., x_test.shape = (60, 400, 400)\n",
        "# y_test  : 84 sounds-labels\n",
        "\n",
        "print('x_train.shape=', x_train.shape) #252 elements\n",
        "print('x_test.shape=', x_test.shape)   #84 elements\n",
        "\n",
        "\n",
        "#Convert to float\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#Normalize inputs from [0; 255] to [0; 1]\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "#Global values\n",
        "num_classes = 12\n",
        "\n",
        "TRAIN_SIZE = x_train.shape[0]\n",
        "TEST_SIZE = x_test.shape[0]\n",
        "\n",
        "\n",
        "NB_VERSIONS_TRAIN = TRAIN_SIZE / num_classes\n",
        "NB_VERSIONS_TEST = TEST_SIZE / num_classes\n",
        "\n",
        "# Les labels sont un tableau où chaque élément correspond au label de l'image\n",
        "# d'indice correspondant\n",
        "y_train = np.empty(TRAIN_SIZE)\n",
        "for i in range(TRAIN_SIZE):\n",
        "  y_train[i] = i//NB_VERSIONS_TRAIN\n",
        "  \n",
        "y_test = np.empty(TEST_SIZE)\n",
        "for i in range(TEST_SIZE):\n",
        "  y_test[i] = i//NB_VERSIONS_TEST\n",
        "\n",
        "print(len(x_train[0][0]))\n",
        "print(len(x_test))  \n",
        "  \n",
        "print('y_train.shape=', y_train.shape)\n",
        "print('y_test.shape=', y_test.shape)\n",
        "\n",
        "\n",
        "#Convert class vectors to binary class matrices (\"one hot encoding\")\n",
        "## Doc : https://keras.io/utils/#to_categorical\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape= (252, 400, 400, 1)\n",
            "x_test.shape= (84, 400, 400, 1)\n",
            "400\n",
            "84\n",
            "y_train.shape= (252,)\n",
            "y_test.shape= (84,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h3Z7-P81KBZg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Convolutional Neural Network\n",
        "\n",
        "Now we need to create and compile the CNN that will classify our datas."
      ]
    },
    {
      "metadata": {
        "id": "VTQtbNLt9Th6",
        "colab_type": "code",
        "outputId": "681fa7b7-9fd1-4331-c78d-f56c8e18ddd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Creation of the Convolutional Network\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "#model.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "model.add(Dropout(0.75))\n",
        "model.add(Flatten(data_format=None))\n",
        "model.add(Dense(units=128, activation='sigmoid'))\n",
        "model.add(Dense(units=12, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs= 30, batch_size=6)\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=6)\n",
        "print('loss =', loss_and_metrics[0],'accuracy =', loss_and_metrics[1]);\n",
        "\n",
        "model.summary();\n",
        "classes = model.predict(x_test, batch_size=72);\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 252 samples, validate on 84 samples\n",
            "Epoch 1/30\n",
            "252/252 [==============================] - 127s 506ms/step - loss: 3.0955 - acc: 0.0754 - val_loss: 2.6030 - val_acc: 0.0833\n",
            "Epoch 2/30\n",
            "252/252 [==============================] - 124s 494ms/step - loss: 2.5396 - acc: 0.0833 - val_loss: 2.4888 - val_acc: 0.0833\n",
            "Epoch 3/30\n",
            "252/252 [==============================] - 125s 495ms/step - loss: 2.5189 - acc: 0.0437 - val_loss: 2.4870 - val_acc: 0.0833\n",
            "Epoch 4/30\n",
            "252/252 [==============================] - 123s 488ms/step - loss: 2.5089 - acc: 0.0516 - val_loss: 2.4875 - val_acc: 0.0833\n",
            "Epoch 5/30\n",
            "252/252 [==============================] - 124s 491ms/step - loss: 2.5104 - acc: 0.0595 - val_loss: 2.4877 - val_acc: 0.0833\n",
            "Epoch 6/30\n",
            "252/252 [==============================] - 123s 489ms/step - loss: 2.5107 - acc: 0.0595 - val_loss: 2.4868 - val_acc: 0.0833\n",
            "Epoch 7/30\n",
            "252/252 [==============================] - 123s 488ms/step - loss: 2.5115 - acc: 0.0635 - val_loss: 2.4888 - val_acc: 0.0833\n",
            "Epoch 8/30\n",
            "252/252 [==============================] - 124s 492ms/step - loss: 2.5001 - acc: 0.0317 - val_loss: 2.4879 - val_acc: 0.0833\n",
            "Epoch 9/30\n",
            "252/252 [==============================] - 123s 489ms/step - loss: 2.5060 - acc: 0.0635 - val_loss: 2.4863 - val_acc: 0.0833\n",
            "Epoch 10/30\n",
            "252/252 [==============================] - 125s 496ms/step - loss: 2.5109 - acc: 0.0437 - val_loss: 2.4862 - val_acc: 0.0833\n",
            "Epoch 11/30\n",
            "252/252 [==============================] - 123s 487ms/step - loss: 2.5091 - acc: 0.0476 - val_loss: 2.4874 - val_acc: 0.0833\n",
            "Epoch 12/30\n",
            "252/252 [==============================] - 123s 489ms/step - loss: 2.5069 - acc: 0.0635 - val_loss: 2.4871 - val_acc: 0.0833\n",
            "Epoch 13/30\n",
            "252/252 [==============================] - 125s 495ms/step - loss: 2.5070 - acc: 0.0556 - val_loss: 2.4864 - val_acc: 0.0833\n",
            "Epoch 14/30\n",
            "252/252 [==============================] - 123s 488ms/step - loss: 2.5096 - acc: 0.0278 - val_loss: 2.4869 - val_acc: 0.0833\n",
            "Epoch 15/30\n",
            "252/252 [==============================] - 125s 495ms/step - loss: 2.5087 - acc: 0.0635 - val_loss: 2.4866 - val_acc: 0.0833\n",
            "Epoch 16/30\n",
            "252/252 [==============================] - 124s 492ms/step - loss: 2.5144 - acc: 0.0437 - val_loss: 2.4881 - val_acc: 0.0833\n",
            "Epoch 17/30\n",
            "252/252 [==============================] - 124s 492ms/step - loss: 2.5111 - acc: 0.0754 - val_loss: 2.4870 - val_acc: 0.0833\n",
            "Epoch 18/30\n",
            "252/252 [==============================] - 124s 494ms/step - loss: 2.5106 - acc: 0.0595 - val_loss: 2.4872 - val_acc: 0.0833\n",
            "Epoch 19/30\n",
            "120/252 [=============>................] - ETA: 1:01 - loss: 2.4989 - acc: 0.0583"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nhAg7J3CBN1S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss and Accuracy\n"
      ]
    },
    {
      "metadata": {
        "id": "Azp5XerEBG1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "i = 0;\n",
        "print('x_test.shape', x_test.shape, 'dtype', x_test.dtype)\n",
        "print('y[{}]={}'.format(i, y_test[i]))\n",
        "plt.imshow(x_test[i,:].reshape(400,400), cmap = matplotlib.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.gcf().clear()\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yogo6mitK4sW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "\n",
        "1) loss : 0.2628 - acc: 0.9722 - val_loss : 3.9648 - val_acc: 0.0278\n",
        "\n",
        "2) après ajout de dropout :\n",
        "loss: 0.0153 - acc: 1.0000 - val_loss: 4.3796 - val_acc: 0.2222\n",
        "\n",
        "3) après augmentation des données :\n",
        "loss: 0.0141 - acc: 1.0000 - val_loss: 4.2034 - val_acc: 0.2500\n",
        "\n",
        "4) encore augmentation des données :\n",
        "c'est pire :'(\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "ajouter du Dropout ?\n",
        "\n",
        "PROBLEME : pas la bonne image affichée"
      ]
    }
  ]
}