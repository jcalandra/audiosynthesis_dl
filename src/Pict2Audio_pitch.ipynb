{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pict2Audio_pitch",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcalandra/audiosynthesis_dl/blob/master/src/Pict2Audio_pitch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2bDMFdrIp9f",
        "colab_type": "text"
      },
      "source": [
        "# Pict2Audio : A Neural Network that associates Pictures to Audio Descriptors\n",
        "\n",
        "## Pict2Audio_pitch\n",
        "This is the independant code for the pitch neural network. This code aims to test the efficiency of every possibilities of databases and architectures for this neural network.\n",
        "\n",
        "**You will find the multilabel version at the following link :**\n",
        "[https://colab.research.google.com/drive/1_ZTdR2CG_eekUUtqAG9Bqa7RixHL8v93](https://colab.research.google.com/drive/1_ZTdR2CG_eekUUtqAG9Bqa7RixHL8v93)\n",
        "\n",
        "## Importation of the libraries\n",
        "\n",
        "First, we need to import all the package and libraries necessary to run the code.\n",
        "\n",
        "The backend Tensorflow is used with the library Keras to implement the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZFRXiCWomB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, BatchNormalization,Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing import image\n",
        "print('tensorflow:', tf.__version__)\n",
        "print('keras:', keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdsYKWFeH6kq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Importation of the Dataset :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK-IhlQAnK21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, import git repository\n",
        "! git clone https://github.com/jcalandra/audiosynthesis_dl.git\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdu2H9vq-PzX",
        "colab_type": "text"
      },
      "source": [
        "Then, run the following script in the same environment :\n",
        "\n",
        "https://colab.research.google.com/drive/1lJELWVC4DmQSNOw0fat4KrzTo_GmVbpZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5LsHZgZDdGX",
        "colab_type": "text"
      },
      "source": [
        "To avoid downloading a heavy set of data on the computer, I chose to generate the pictures directly and load them from google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwmaXP_6ClUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqP7MVGtF4EG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## LOADING THE BASELINE\n",
        "\n",
        "baseline = cv2.imread('audiosynthesis_dl/data/base_quadrillage.png')\n",
        "#cv2_imshow(baseline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aroCrfQoFzVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GLOBAL VALUES\n",
        "\n",
        "NB_PITCH = 12\n",
        "NB_COLOR = 8\n",
        "NB_THICK = 3\n",
        "NB_CLASS = NB_PITCH\n",
        "PICT_WIDTH = 400\n",
        "NB_CHANNEL = 3\n",
        "LINE_WIDTH = PICT_WIDTH//NB_PITCH #33\n",
        "\n",
        "NB_CARACTERISTICS = 1\n",
        "\n",
        "# number of versions for a same pitch\n",
        "NB_VERSION_TRAIN = 2\n",
        "NB_VERSION_VALIDATION = 1\n",
        "\n",
        "# number of trained and validation pictures \n",
        "NB_TRAIN = NB_THICK*NB_COLOR*NB_PITCH*NB_VERSION_TRAIN\n",
        "NB_VALIDATION = NB_THICK*NB_COLOR*NB_PITCH*NB_VERSION_VALIDATION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkwWpB1jLQcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## PICTURE GENERATION\n",
        "\n",
        "# generating exactly the same path line for every color, pitch and thickness\n",
        "\n",
        "def generate_pict(nb_version_pict, folder, outline_value, pic_type): # pic_type = [PITCH, THICK, COLOR] with 0 if no, 1 if yes\n",
        "  \"\"\" creates nb_pictures of pitch-lines and saves them permanently in the folder img_'folder'.\n",
        "      Folder has to be a string and nb_pict is an integer.\"\"\"\n",
        "  \n",
        "  # default values if pitches, thicknesses and/or colors don't change\n",
        "  height = (3%NB_PITCH)*LINE_WIDTH + (LINE_WIDTH + 5)//2\n",
        "  thickness = 6\n",
        "  sat = hue = val = 0\n",
        "  # default indices\n",
        "  pitch_ind = 0\n",
        "  thick_ind = 0\n",
        "  color_ind = 0\n",
        "  \n",
        "  # beginning value for thickness if there is a variation of thickness\n",
        "  if (pic_type[1] == 1):\n",
        "    thickness = 12\n",
        "  # tab of color values if there is a variation of colors\n",
        "  hueval_tab = [[(i*180)/(NB_COLOR//2), 90] for i in range(NB_COLOR//2)] + [[(i*180)/(NB_COLOR//2), 210] for i in range(NB_COLOR//2)]\n",
        "  #tab of available thicknesses if there is a variotion of thicknesses\n",
        "  thickness_tab = [2,7,12]\n",
        "  global_line_path = np.empty((PICT_WIDTH),dtype=int)\n",
        "  \n",
        "  for i in range (nb_version_pict) :\n",
        "    \n",
        "    # generation of lines path :   \n",
        "    outline = outline_value \n",
        "    delta = (LINE_WIDTH - (thickness*2))//2 + outline    \n",
        "    variation = np.random.randint(0,delta - outline)   # the line begin at a random point in delta\n",
        "    intervalle_max = np.random.randint(2,50)           # interval allowed to keep the same height between each variation\n",
        "    \n",
        "    for l in range(PICT_WIDTH):\n",
        "      intervalle = np.random.randint(1,intervalle_max) # to avoid a sharp variation, we keep the same variation height for each 'intervalle'\n",
        "      if (l%intervalle == 0) :                         # if we want to change the height of the line\n",
        "        tmp_var = np.random.randint(-1, 2)             # each variation is an increase or a decrease of 1 (or same height)\n",
        "        if abs(variation + tmp_var) < delta :\n",
        "          variation = variation + tmp_var\n",
        "        else :\n",
        "          variation = variation             \n",
        "      global_line_path[l] = PICT_WIDTH + variation\n",
        "    \n",
        "    # pitch affiliation :    \n",
        "    for p in range(NB_PITCH):\n",
        "      if (pic_type[0] == 1 ):\n",
        "        pitch_ind = 69 + p #69 is pitch for la440\n",
        "        height = p*LINE_WIDTH + (LINE_WIDTH + 5)//2       \n",
        "      line_path = global_line_path - height - 1\n",
        "\n",
        "      \n",
        "      # generation of the pictures. There are nb_version_pict*NB_COLOR*NB_PITCH pictures :\n",
        "      for m in range(NB_COLOR):  \n",
        "        \n",
        "        # creation of the baseline, quadrilled picture :       \n",
        "        line_image_rgb = baseline.copy()\n",
        "\n",
        "                \n",
        "        # color affiliation :\n",
        "        line_image_hsv = cv2.cvtColor(line_image_rgb, cv2.COLOR_RGB2HSV)\n",
        "        h, s, v = cv2.split(line_image_hsv)\n",
        "      \n",
        "        if( pic_type[2] == 1):\n",
        "          color_ind = m\n",
        "          color = hueval_tab[color_ind]\n",
        "          sat = 150\n",
        "          hue = color[0]\n",
        "          val = color[1]\n",
        "          \n",
        "        #thickness affiliation :\n",
        "        for t in range(NB_THICK): \n",
        "          thick_ind = t\n",
        "          thickness = thickness_tab[thick_ind]\n",
        "\n",
        "          for j in range(PICT_WIDTH):\n",
        "\n",
        "            # creation of the line :\n",
        "            if( line_path[j] > 0 and line_path[j] < 400 ) :\n",
        "              h[line_path[j]][PICT_WIDTH - j - 1] = hue      \n",
        "              s[line_path[j]][PICT_WIDTH - j - 1] = sat \n",
        "              v[line_path[j]][PICT_WIDTH - j - 1] = val \n",
        "\n",
        "            # and its thickness :\n",
        "            for k in range (thickness) :\n",
        "              if( line_path[j] - k > 0 and line_path[j] - k < 400 ) :\n",
        "                h[line_path[j] - k][PICT_WIDTH - j - 1] = hue \n",
        "                s[line_path[j] - k][PICT_WIDTH - j - 1] = sat\n",
        "                v[line_path[j] - k][PICT_WIDTH - j - 1] = val\n",
        "\n",
        "              if( line_path[j] + k > 0 and line_path[j] + k < 400 ) :\n",
        "                h[line_path[j] + k][PICT_WIDTH - j - 1] = hue\n",
        "                s[line_path[j] + k][PICT_WIDTH - j - 1] = sat\n",
        "                v[line_path[j] + k][PICT_WIDTH - j - 1] = val\n",
        "\n",
        "          line_image_hsv = cv2.merge((h,s,v))\n",
        "          line_image = cv2.cvtColor(line_image_hsv, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "\n",
        "          cv2_imshow(line_image) #showing the images for tests\n",
        "          name = 'pitch'+str(pitch_ind)+ '_thick' + str(thick_ind) + '_color' + str(color_ind) +'_'+str(i) + '_'+folder+'.png'\n",
        "          print(name)\n",
        "\n",
        "          # save the picture in google colab :\n",
        "          cv2.imwrite('./audiosynthesis_dl/data/pitch_img/img_'+folder+'/'+ name, line_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO7t2WHPTvD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('[INFO] generating training dataset...')\n",
        "generate_pict(NB_VERSION_TRAIN, 'train',0,[1,1,1])\n",
        "print('[INFO] generating validation dataset...')\n",
        "generate_pict(NB_VERSION_VALIDATION, 'validation',0,[1,1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MVn7xDHGND2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CREATION OF LABELS if not generated while loading the pictures according to their names\n",
        "\n",
        "# Les labels sont un tableau où chaque élément correspond au label de l'image\n",
        "# d'indice correspondant\n",
        "label_train = np.empty(NB_TRAIN)\n",
        "label_validation = np.empty(NB_VALIDATION)\n",
        "\n",
        "def labelisation(label_tab, nb_pict) :\n",
        "  nb_versions = nb_pict / NB_PITCH\n",
        "  for i in range(nb_pict):\n",
        "    label_tab[i] = i//nb_versions\n",
        "  \n",
        "labelisation(label_train, NB_TRAIN)\n",
        "labelisation(label_validation, NB_VALIDATION)\n",
        "\n",
        "print(label_train)\n",
        "\n",
        "print('label_train.shape=', label_train.shape)\n",
        "print('label_validation.shape=', label_validation.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgWQoVexJylW",
        "colab_type": "text"
      },
      "source": [
        "## Loading the datas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl1ulRj9J7GW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## LOADING THE PICTURES\n",
        "\n",
        "#TODO : trouver un moyen d'optimiser\n",
        "\n",
        "img_train = np.empty((0,400,400,NB_CHANNEL))\n",
        "for imgP in sorted(os.listdir( \"./audiosynthesis_dl/data/pitch_img/img_train\")[:]) :\n",
        "  if imgP.split(\".\")[-1] != \"git\" and imgP.split(\".\")[-1] != \"gitignore\":\n",
        "    img = image.load_img( \"./audiosynthesis_dl/data/pitch_img/img_train/\"+imgP, \n",
        "                             target_size=(400, 400),\n",
        "                             color_mode='rgb')\n",
        "    # To input our values in our network Conv2D layer, we need to reshape the \n",
        "    # datasets, i.e., pass from (60, 400, 400) to (60, 400, 400, 1) where 1 is \n",
        "    # the number of channels of our images\n",
        "    img_train = np.concatenate((img_train,np.reshape(img,(1,400,400,NB_CHANNEL))),axis=0)\n",
        "    \n",
        "img_validation = np.empty((0,400,400,NB_CHANNEL))\n",
        "for imgP in sorted(os.listdir( \"./audiosynthesis_dl/data/pitch_img/img_validation\")[:]) :\n",
        "  if imgP.split(\".\")[-1] != \"git\" and imgP.split(\".\")[-1] != \"gitignore\":\n",
        "    img = image.load_img( \"./audiosynthesis_dl/data/pitch_img/img_validation/\"+imgP, \n",
        "                             target_size=(400, 400),\n",
        "                             color_mode='rgb')\n",
        "    img_validation = np.concatenate((img_validation,np.reshape(img,(1,400,400,NB_CHANNEL))),axis=0)\n",
        "\n",
        "print('img_train.shape=', img_train.shape)\n",
        "print('img_validation.shape=', img_validation.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAkgzKXpTorp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CONVERTING THE DATA\n",
        "\n",
        "def converting_data(img_tab) :\n",
        "  \"\"\" convert the data in float between 0 et 1\"\"\"\n",
        "  # Convert to float\n",
        "  img_tab = img_tab.astype('float32')\n",
        "  # Normalize inputs from [0; 255] to [0; 1]\n",
        "  imgnorm_tab = img_tab / 255\n",
        "\n",
        "  return imgnorm_tab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "addzNAZwRqW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## MIXING THE DATA FOR A GOOD LEARNING\n",
        "def shuffle_data(img_tab, label_tab, nb_pict) :\n",
        "  \"\"\" create shuffled tabs of data and corresponding labels \"\"\"\n",
        "  imgnorm_tab = converting_data(img_tab)\n",
        "  print(imgnorm_tab.shape)\n",
        "    \n",
        "  xy_tab = []\n",
        "  for i in range(nb_pict):\n",
        "    xy = [imgnorm_tab[i],label_tab[i]]\n",
        "    xy_tab.append(xy)\n",
        "  \n",
        "  random.shuffle(xy_tab)\n",
        "  \n",
        "  x_tab = np.empty((nb_pict,400,400,NB_CHANNEL))\n",
        "  y_tab = np.empty(nb_pict)\n",
        "  \n",
        "  for i in range(nb_pict):\n",
        "    x_tab[i] = xy_tab[i][0]\n",
        "    y_tab[i] = xy_tab[i][1]\n",
        "      \n",
        "  del imgnorm_tab\n",
        "  del xy_tab\n",
        "  return [x_tab,y_tab]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shNmUjFeetfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.copy(img_train)\n",
        "x_validation = np.copy(img_validation)\n",
        "y_train = np.copy((label_train,NB_CARACTERISTICS))\n",
        "y_validation = np.copy((label_validation,NB_CARACTERISTICS))\n",
        "    \n",
        "[x_train, y_train] = shuffle_data(img_train, label_train, NB_TRAIN)\n",
        "[x_validation, y_validation] = shuffle_data(img_validation, label_validation, NB_VALIDATION)\n",
        "\n",
        "del img_train\n",
        "del img_validation\n",
        "del label_train\n",
        "del label_validation\n",
        "\n",
        "print('x_train.shape=', x_train.shape)\n",
        "print('x_validation.shape=', x_validation.shape)\n",
        "\n",
        "\n",
        "print('y_train.shape=', y_train.shape)\n",
        "print('y_validation.shape=', y_validation.shape)\n",
        "print(y_validation)\n",
        "\n",
        "# Convert class vectors to binary class matrices (\"one hot encoding\")\n",
        "y_train = keras.utils.to_categorical(y_train, NB_CLASS)\n",
        "y_validation = keras.utils.to_categorical(y_validation, NB_CLASS)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Z7-P81KBZg",
        "colab_type": "text"
      },
      "source": [
        "## The Convolutional Neural Network\n",
        "\n",
        "Now we need to create and compile the CNN that will classify our datas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTQtbNLt9Th6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CREATION OF THE NEURAL NETWORK\n",
        "\n",
        "model_pitch = Sequential()\n",
        "\n",
        "model_pitch.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(MaxPooling2D(pool_size=(3,3), strides=None, padding='valid', data_format=None))\n",
        "              \n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_pitch.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_pitch.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "model_pitch.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "# first (and only) set of FC => RELU layers\n",
        "model_pitch.add(Flatten())\n",
        "model_pitch.add(Dense(1024))\n",
        "model_pitch.add(Activation(\"relu\"))\n",
        "model_pitch.add(BatchNormalization())\n",
        "model_pitch.add(Dropout(rate=0.5))\n",
        " \n",
        "# use a *sigmoid* activation for multi-label classification\n",
        "model_pitch.add(Dense(NB_CLASS))\n",
        "model_pitch.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model_pitch.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "hist = model_pitch.fit(x_train, y_train, validation_data=(x_validation, y_validation), epochs= 100, batch_size=32)\n",
        "loss_and_metrics = model_pitch.evaluate(x_validation, y_validation, batch_size=32)\n",
        "print('loss =', loss_and_metrics[0],'accuracy =', loss_and_metrics[1]);\n",
        "\n",
        "model_pitch.summary();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yogo6mitK4sW",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "pitch(12)+thick(6)+color(8)\n",
        "OK with 360+120 samples on epoch=100 batch=32 :\n",
        "\n",
        "loss = 7.312692999524491e-06 accuracy = 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhAg7J3CBN1S",
        "colab_type": "text"
      },
      "source": [
        "## Graphics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbZhQDs2WRYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azp5XerEBG1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS8GpqWVrRWS",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F68w_o8rQlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## With the img_test folder :\n",
        "\n",
        "NB_TEST = 0\n",
        "img_test = np.empty((0,400,400,NB_CHANNEL))\n",
        "for imgP in sorted(os.listdir( \"./audiosynthesis_dl/data/pitch_img/img_test\")[:]) :\n",
        "  if imgP.split(\".\")[-1] != \"git\":\n",
        "    NB_TEST += 1\n",
        "    img = image.load_img( \"./audiosynthesis_dl/data/pitch_img/img_test/\"+imgP, \n",
        "                             target_size=(400, 400),\n",
        "                             color_mode='rgb')\n",
        "    img_test = np.concatenate((img_test,np.reshape(img,(1,400,400,NB_CHANNEL))),axis=0)\n",
        "\n",
        "label_test = np.empty(NB_TEST)    \n",
        "labelisation(label_test, NB_TEST)\n",
        "\n",
        "[x_test, label_test] = shuffle_data(img_test, label_test, NB_TEST)    \n",
        "    \n",
        "y_test = model_pitch.predict_classes(x_test)\n",
        "# show the inputs and predicted outputs\n",
        "for i in range(len(x_test)):\n",
        "\tprint(\"y_test[%s] = %s\" % (i, y_test[i]))\n",
        "  \n",
        "def evaluate(label_tab, y_tab, nb_pict) :\n",
        "  cpt = 0\n",
        "  for i in range(nb_pict):\n",
        "    if(label_tab[i] == y_tab[i]) :\n",
        "      cpt += 1\n",
        "  value = cpt/nb_pict\n",
        "  print(\"Accuracy of prediction of the test dataset :\", value)\n",
        "  \n",
        "evaluate(label_test, y_test, NB_TEST)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGdk_H2okkPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1;\n",
        "print('x_test.shape', x_test.shape, 'dtype', x_test.dtype)\n",
        "print('y[{}]={}'.format(i, y_test[i]))\n",
        "plt.imshow(x_test[i,:].reshape(400,400,3), cmap = matplotlib.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.gcf().clear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijNJKy0e76Pf",
        "colab_type": "text"
      },
      "source": [
        "## Sound synthesis according to the labelisation\n",
        "\n",
        "full independant code here : https://colab.research.google.com/drive/1KzM-NMSlj87XU_--cifmipAuYd57zZc4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU3JnGehU915",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math as m\n",
        "import IPython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL2cUtdjbSFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_CHANNEL = 1\n",
        "volume = 0.5     # range [0.0, 1.0]\n",
        "fs = 44100       # sampling rate, Hz, must be integer\n",
        "duration = 4.0   # in seconds, may be float\n",
        "f0 = 440.0        # sine frequency, Hz, may be float\n",
        "label = 0 # default value, take the one given by the neural network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKQOZlLxVcRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sinusoid(label, duration) :\n",
        "  \"\"\" generate an audio of a sinusoid \"\"\"\n",
        "  samples = []\n",
        "  fm  =  (2**(label/12))*f0\n",
        "  # generate samples\n",
        "  for i in range(int(fs*duration)) :\n",
        "    t = i/fs # seconds\n",
        "    a = 2*m.pi*fm*t # radians\n",
        "    v = volume*m.sin(a)\n",
        "    samples.append(v)\n",
        "  return samples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plvUClchyeHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "label_i = y_test[i]\n",
        "\n",
        "sin = sinusoid(label_i, duration)\n",
        "IPython.display.Audio(sin, rate=fs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}