{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pict2Audio_son.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcalandra/audiosynthesis_dl/blob/master/src/Pict2Audio_son.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIwfDj1fPejP",
        "colab_type": "text"
      },
      "source": [
        "# Pict2Audio : A Neural Network that associates Pictures to Audio Descriptors\n",
        "This project consists in associating a sound with one or more characteristics defined by audio descriptors with a picture drawn by a composer . The long-term goal is to allow the composer to develop his own composition graphic language in order to associate it with sounds from some effects banks.\n",
        "\n",
        " \n",
        " ## Sound classification\n",
        "\n",
        "In this study, I'm interested into pitch, volume and tone color features, and I will propose as input to a Convolutional Neural Network trained for classification a database composed of :\n",
        "* sounds from the audio libraries of the composer,\n",
        "* three labels for each audio, corresponding to the features in the labels associated to sound features. \n",
        "\n",
        "## Importation of the python librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr97BJmoiBhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('[INFO] loading the librairies...')\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers, losses, activations, models\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import callbacks\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dense, Input, Dropout, Activation, BatchNormalization, Conv2D, MaxPooling2D, GlobalMaxPool2D, Flatten\n",
        "print('tensorflow:', tf.__version__)\n",
        "print('keras:', keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSINoJlQtLL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/jcalandra/audiosynthesis_dl.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mmf0iphCiOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_PITCH = 12\n",
        "NB_THICK = 5\n",
        "NB_COLOR = 8\n",
        "NB_CARACTERISTICS = 4\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "INPUT_LENGTH = SAMPLE_RATE*4\n",
        "batch_size = 32\n",
        "NB_MELS = 320"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9fNvQvn6dhw",
        "colab_type": "text"
      },
      "source": [
        "## Processing MelSpectrograms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g75n7qw6ClnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_audio_mel(audio, sample_rate=SAMPLE_RATE):\n",
        "  \"\"\" Generate a mel spectrogram in dB given an `audio` \"\"\"\n",
        "  MAX = 50\n",
        "  mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels= NB_MELS)\n",
        "  mel_db = (librosa.power_to_db(mel_spec, ref=MAX) + 80)/80\n",
        "  #TODO :trouver une valeur adaptÃ©e pour MAX\n",
        "  return mel_db\n",
        "\n",
        "\n",
        "def load_audio_file(file_path, input_length=INPUT_LENGTH):\n",
        "  \"\"\" Extend/Reduce an audio `file_path` to the given `input_lengh` then generate its mel spectrogram \"\"\"\n",
        "  data = librosa.core.load(file_path, sr=SAMPLE_RATE)[0] #, sr=16000\n",
        "  if len(data)>input_length:\n",
        "    max_offset = len(data)-input_length       \n",
        "    offset = np.random.randint(max_offset)     \n",
        "    data = data[offset:(input_length+offset)]          \n",
        "  else:\n",
        "    if input_length > len(data):\n",
        "      max_offset = input_length - len(data)\n",
        "      offset = np.random.randint(max_offset)\n",
        "    else:\n",
        "      offset = 0           \n",
        "      data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\") \n",
        "      #note : maybe adding silence could be more accurate depending of the type of sound in input\n",
        "  data = preprocess_audio_mel(data)\n",
        "  return data\n",
        " \n",
        "  \n",
        "def generate_mels(snd_tab):\n",
        "  \"\"\" Put and reshape the mel spectrograms in a tab `snd_tab` \"\"\"\n",
        "  x_tab = np.empty((len(snd_tab),NB_MELS,126, 1))\n",
        "  for i in range(len(snd_tab)) :\n",
        "    x_tab_i = np.reshape(load_audio_file(snd_tab[i]),(1,NB_MELS,126,1))\n",
        "    x_tab[i] = x_tab_i\n",
        "  return x_tab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJvbujZa3cfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snd_train = glob.glob(\"./audiosynthesis_dl/data/bdd_snd/snd_train/*.wav\") #train_files\n",
        "snd_validation = glob.glob(\"./audiosynthesis_dl/data/bdd_snd/snd_validation/*.wav\") \n",
        "\n",
        "NB_TRAIN = len(snd_train)\n",
        "NB_VALIDATION = len(snd_validation)\n",
        "\n",
        "print(snd_train)\n",
        "print(NB_TRAIN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R1Jw_-FA4ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('[INFO] generation of train mel spectrograms...')\n",
        "x_train = generate_mels(snd_train)\n",
        "print('[INFO] generation of validation mel spectrograms...')\n",
        "x_validation = generate_mels(snd_validation)\n",
        "\n",
        "# TODO : il faut des valeurs positives non ?\n",
        "print(x_train)\n",
        "print(len(x_train))\n",
        "\n",
        "print('x_train.shape =', x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0syYTLq9581h",
        "colab_type": "text"
      },
      "source": [
        "## Processing Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDLIqYH4AFSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_label_tab(sndPath_tab):\n",
        "  \"\"\" Create a tab of labels considering a tab of filenames filenames_tab \"\"\"\n",
        "  labels_tab = []\n",
        "  for sndP in sndPath_tab:\n",
        "    snd = sndP.split('.')[0]\n",
        "    l = snd.split(\"-\")\n",
        "    labels = l[:NB_CARACTERISTICS]\n",
        "    labels_tab.append(labels)\n",
        "  return labels_tab\n",
        "\n",
        "def gen_features_tabs(y_tab, y_feature, i_feature) :\n",
        "  \"\"\" Generate a tab containing the labels for one specific feature \"\"\"\n",
        "  for i in range(len(y_tab)) :\n",
        "    y_feature[i] = y_tab[i][i_feature]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPVUidLiB5z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sndPath_train = os.listdir( \"./audiosynthesis_dl/data/bdd_snd/snd_train\")[:]\n",
        "sndPath_validation = os.listdir( \"./audiosynthesis_dl/data/bdd_snd/snd_validation\")[:]\n",
        "\n",
        "label_train = create_label_tab(sndPath_train)\n",
        "label_validation = create_label_tab(sndPath_validation)\n",
        "\n",
        "print(label_train)\n",
        "print(label_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9HsIDyg-HX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label2int(label_tab):\n",
        "  \"\"\"Converts the label tab into int between 0 and n \"\"\"\n",
        "  y_tab = []\n",
        "  for i in range(len(label_tab)):\n",
        "    label_tab_i = []\n",
        "    \n",
        "    # Pitch labelisation\n",
        "    pitch = int(label_tab[i][1]) - 69\n",
        "    \n",
        "    # Thick labelisation\n",
        "    label_thick = label_tab[i][3]\n",
        "    if(label_thick == 'low'):\n",
        "      thick = 0\n",
        "    elif (label_thick == 'med'):\n",
        "      thick = 1\n",
        "    else :\n",
        "      thick = 2\n",
        "      \n",
        "    # Color labelisation\n",
        "    label_color = label_tab[i][0].split('_')[0]\n",
        "    if (label_color == 'brass'):\n",
        "      color = 0\n",
        "    elif (label_color == 'guitar'):\n",
        "      color = 1\n",
        "    elif (label_color == 'keyboard'):\n",
        "      color = 2\n",
        "    elif (label_color == 'organ'):\n",
        "      color = 3\n",
        "    elif (label_color == 'string'):\n",
        "      color = 4\n",
        "    elif (label_color == 'vocal'):\n",
        "      color = 5\n",
        "    elif (label_color == 'flute'):\n",
        "      color = 6\n",
        "    else :\n",
        "      color = 7\n",
        "      \n",
        "    label_tab_i.append(color)\n",
        "    label_tab_i.append(pitch)\n",
        "    label_tab_i.append(thick)\n",
        "    y_tab.append(label_tab_i)  \n",
        "  return y_tab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Fp-QYaBzE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('[INFO] convertion of train labels into int...')\n",
        "y_train = label2int(label_train)\n",
        "print('[INFO] convertion of validation labels into int...')\n",
        "y_validation = label2int(label_validation)\n",
        "\n",
        "print(y_train)\n",
        "print(y_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FauZ2Uao_bJY",
        "colab_type": "text"
      },
      "source": [
        "## Plot the Mel Spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FQ-EAsb69vS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## PLOT THE MELSPECTROGRAM\n",
        "\n",
        "#TODO : trouver un meilleur affichage car on croirait avoir les mÃªme valeurs\n",
        "#pour low et med (les valeurs des spectrogrammes sont effectivement diffÃ©rentes \n",
        "#- voir les Ã©chelles) ou alors les tons pour low deviennent dans les bleus qd MAX >env60\n",
        "\n",
        "ind1 = './audiosynthesis_dl/data/bdd_snd/snd_train/guitar_acoustic_021-069-025-low.wav'\n",
        "# low corresponds to -22dB compared to med\n",
        "data_base = load_audio_file(ind1)\n",
        "fig = plt.figure(figsize=(7, 4))\n",
        "plt.title('Mel Spectrogram : %s ' % (ind1))\n",
        "librosa.display.specshow(data_base, sr=SAMPLE_RATE, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+02.0f dB')\n",
        "\n",
        "ind2 = './audiosynthesis_dl/data/bdd_snd/snd_train/guitar_acoustic_021-069-025-med.wav'\n",
        "data_base = load_audio_file(ind2)\n",
        "fig = plt.figure(figsize=(7, 4))\n",
        "plt.title('Mel Spectrogram : %s ' % (ind2))\n",
        "librosa.display.specshow(data_base, sr=SAMPLE_RATE, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+02.0f dB')\n",
        "\n",
        "\n",
        "ind3 = './audiosynthesis_dl/data/bdd_snd/snd_train/guitar_acoustic_021-069-025-high.wav'\n",
        "# high corresponds to +10dB compared to med\n",
        "data_base = load_audio_file(ind3)\n",
        "fig = plt.figure(figsize=(7, 4))\n",
        "plt.title('Mel Spectrogram : %s ' % (ind3))\n",
        "librosa.display.specshow(data_base, sr=SAMPLE_RATE, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+02.0f dB')\n",
        "#note : there is saturation due to the modification of the medium sound"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N44xF3bFQpff",
        "colab_type": "text"
      },
      "source": [
        "## Training the Neural Networks\n",
        "\n",
        "### Pitch Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jINv9K717f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GENERATION OF THE PITCH FEATURES TABS\n",
        "\n",
        "print('[INFO] generation of the pitch features tabs...')\n",
        "y_train_pitch = np.empty(NB_TRAIN)\n",
        "y_validation_pitch = np.empty(NB_VALIDATION)\n",
        "\n",
        "gen_features_tabs(y_train, y_train_pitch, 1)\n",
        "gen_features_tabs(y_validation, y_validation_pitch, 1)\n",
        "\n",
        "print(y_train_pitch)\n",
        "print(y_validation_pitch)\n",
        "\n",
        "# Convert class vectors to binary class matrices (\"one hot encoding\")\n",
        "print('[INFO] converting pitch class vectors...')\n",
        "y_train_pitch = keras.utils.to_categorical(y_train_pitch, NB_PITCH)\n",
        "y_validation_pitch = keras.utils.to_categorical(y_validation_pitch, NB_PITCH)\n",
        "\n",
        "print('y_train_pitch.shape =', y_train_pitch.shape)\n",
        "print('y_validation_pitch.shape =', y_validation_pitch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHCMH2IqLLp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CREATION OF THE PITCH NEURAL NETWORK\n",
        "\n",
        "model_pitch = Sequential()\n",
        "\n",
        "model_pitch.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(MaxPooling2D(pool_size=(3,3), strides=None, padding='valid', data_format=None))\n",
        "              \n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_pitch.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_pitch.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "model_pitch.add(Dropout(rate=0.2))\n",
        "\n",
        "model_pitch.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_pitch.add(BatchNormalization(axis=-1))\n",
        "model_pitch.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "model_pitch.add(Dropout(rate=0.2))\n",
        "\n",
        "# first (and only) set of FC => RELU layers\n",
        "model_pitch.add(Flatten())\n",
        "model_pitch.add(Dense(1024))\n",
        "model_pitch.add(Activation(\"relu\"))\n",
        "model_pitch.add(BatchNormalization())\n",
        "model_pitch.add(Dropout(rate=0.5))\n",
        " \n",
        "# use a *sigmoid* activation for multi-label classification\n",
        "model_pitch.add(Dense(NB_PITCH))\n",
        "model_pitch.add(Activation('softmax'))\n",
        "\n",
        "opt_pitch = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model_pitch.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "cb = callbacks.ModelCheckpoint('model_pitchsnd.h5', save_best_only=True, period=1)\n",
        "#model.load_weights(\"baseline_cnn.h5\")\n",
        "\n",
        "hist_pitch = model_pitch.fit(x_train, y_train_pitch, validation_data=(x_validation, y_validation_pitch), epochs=100, batch_size = 32, callbacks=[cb])\n",
        "loss_and_metrics_pitch = model_pitch.evaluate(x_validation, y_validation_pitch, batch_size=32)\n",
        "print('loss =', loss_and_metrics_pitch[0],'accuracy =', loss_and_metrics_pitch[1]);\n",
        "\n",
        "model_pitch.summary();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo_5oY-xcvUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hist_pitch.history['acc'])\n",
        "plt.plot(hist_pitch.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['pitch train', 'pitch validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(hist_pitch.history['loss'])\n",
        "plt.plot(hist_pitch.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['pitch train', 'pitch validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "del y_train_pitch\n",
        "del y_validation_pitch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgTDhGFCEUVH",
        "colab_type": "text"
      },
      "source": [
        "rÃ©sults : \n",
        "* C2D(32)+2xC2D(64)+2xC2D(128)+Dropout(0.2)+2xC2D(264)+Dropout(0.2)+dense(1024)+Dropout(0.5)\n",
        "* epoch = 100, batch size = 32, opt adam\n",
        "* loss: 4.9768e-05 - acc: 1.0000 - val_loss: 0.6577 - val_acc: 0.9310"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8-NZU445n_C",
        "colab_type": "text"
      },
      "source": [
        "### Thick Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5YPTaomBdqP",
        "colab_type": "code",
        "outputId": "2b7548b5-679c-4b18-95cd-8a4f876acc11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "## GENERATION OF THE THICK==VOLUME FEATURES TABS\n",
        "\n",
        "print('[INFO] generation of the thick features tabs...')\n",
        "y_train_thick = np.empty(NB_TRAIN)\n",
        "y_validation_thick = np.empty(NB_VALIDATION)\n",
        "\n",
        "gen_features_tabs(y_train, y_train_thick, 2)\n",
        "gen_features_tabs(y_validation, y_validation_thick, 2)\n",
        "\n",
        "print('[INFO] converting thick class vectors...')\n",
        "y_train_thick = keras.utils.to_categorical(y_train_thick, NB_THICK)\n",
        "y_validation_thick = keras.utils.to_categorical(y_validation_thick, NB_THICK)\n",
        "\n",
        "print('y_train_thick.shape =', y_train_thick.shape)\n",
        "print('y_validation_thick.shape =', y_validation_thick.shape)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] generation of the thick features tabs...\n",
            "[INFO] converting thick class vectors...\n",
            "y_train_thick.shape = (282, 5)\n",
            "y_validation_thick.shape = (87, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSZBiMQw5rQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CREATION OF THE THICK NEURAL NETWORK\n",
        "\n",
        "model_thick = Sequential()\n",
        "\n",
        "model_thick.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_thick.add(BatchNormalization(axis=-1))\n",
        "model_thick.add(MaxPooling2D(pool_size=(3,3), strides=None, padding='valid', data_format=None))\n",
        "              \n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_thick.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_thick.add(BatchNormalization(axis=-1))\n",
        "model_thick.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_thick.add(BatchNormalization(axis=-1))\n",
        "model_thick.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_thick.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_thick.add(BatchNormalization(axis=-1))\n",
        "model_thick.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_thick.add(BatchNormalization(axis=-1))\n",
        "model_thick.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "model_thick.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_thick.add(BatchNormalization(axis=-1))\n",
        "model_thick.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_thick.add(BatchNormalization(axis=-1))\n",
        "model_thick.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "# first (and only) set of FC => RELU layers\n",
        "model_thick.add(Flatten())\n",
        "model_thick.add(Dense(1024))\n",
        "model_thick.add(Activation(\"relu\"))\n",
        "model_thick.add(BatchNormalization())\n",
        " \n",
        "# use a *sigmoid* activation for multi-label classification\n",
        "model_thick.add(Dense(NB_THICK))\n",
        "model_thick.add(Activation('softmax'))\n",
        "\n",
        "model_thick.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "cb = callbacks.ModelCheckpoint('model_thicksnd.h5', save_best_only=True, period=1)\n",
        "#model.load_weights(\"baseline_cnn.h5\")\n",
        "\n",
        "hist_thick = model_thick.fit(x_train, y_train_thick, validation_data=(x_validation, y_validation_thick), epochs=100, batch_size = 32, callbacks=[cb])\n",
        "loss_and_metrics_thick = model_thick.evaluate(x_validation, y_validation_thick, batch_size=32)\n",
        "print('loss =', loss_and_metrics_thick[0],'accuracy =', loss_and_metrics_thick[1]);\n",
        "\n",
        "model_thick.summary();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix4NqgkG7XZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hist_thick.history['acc'])\n",
        "plt.plot(hist_thick.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['thick train', 'thick validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(hist_thick.history['loss'])\n",
        "plt.plot(hist_thick.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['thick train', 'thick validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "del y_train_thick\n",
        "del y_validation_thick"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5EG8IULfTsv",
        "colab_type": "text"
      },
      "source": [
        "rÃ©sults : \n",
        "* 2xC2D(32)+2xC2D(64)+3xC2D(128)+2xC2D(264)+dense(1024)\n",
        "* epoch = 100, batch size = 32, opt adam\n",
        "* loss: 0.5038 - acc: 0.6064 - val_loss: 0.5148 - val_acc: 0.6552"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cbxuREB8xvT",
        "colab_type": "text"
      },
      "source": [
        "### Color Convolutional Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9DMVK8_Burd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GENERATION OF THE COLOR==TONE FEATURES TABS\n",
        "\n",
        "print('[INFO] generation of the color features tabs...')\n",
        "y_train_color = np.empty(NB_TRAIN)\n",
        "y_validation_color = np.empty(NB_VALIDATION)\n",
        "\n",
        "gen_features_tabs(y_train, y_train_color, 0)\n",
        "gen_features_tabs(y_validation, y_validation_color, 0)\n",
        "\n",
        "print('[INFO] converting color class vectors...')\n",
        "y_train_color = keras.utils.to_categorical(y_train_color, NB_COLOR)\n",
        "y_validation_color = keras.utils.to_categorical(y_validation_color, NB_COLOR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPn76_jH9DUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CREATION OF THE COLOR NEURAL NETWORK\n",
        "\n",
        "model_color = Sequential()\n",
        "\n",
        "model_color.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_color.add(BatchNormalization(axis=-1))\n",
        "model_color.add(MaxPooling2D(pool_size=(3,3), strides=None, padding='valid', data_format=None))\n",
        "              \n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_color.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_color.add(BatchNormalization(axis=-1))\n",
        "model_color.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_color.add(BatchNormalization(axis=-1))\n",
        "model_color.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "# (CONV => RELU) * 2 => POOL\n",
        "model_color.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_color.add(BatchNormalization(axis=-1))\n",
        "model_color.add(Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_color.add(BatchNormalization(axis=-1))\n",
        "model_color.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "model_color.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_color.add(BatchNormalization(axis=-1))\n",
        "model_color.add(Conv2D(filters=256, kernel_size=(3,3), strides=1, padding='valid', activation='relu'))\n",
        "model_color.add(BatchNormalization(axis=-1))\n",
        "model_color.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))\n",
        "\n",
        "# first (and only) set of FC => RELU layers\n",
        "model_color.add(Flatten())\n",
        "model_color.add(Dense(1024))\n",
        "model_color.add(Activation(\"relu\"))\n",
        "model_color.add(BatchNormalization())\n",
        "model_color.add(Dropout(rate=0.5))\n",
        " \n",
        "# use a *sigmoid* activation for multi-label classification\n",
        "model_color.add(Dense(NB_COLOR))\n",
        "model_color.add(Activation('softmax'))\n",
        "\n",
        "opt_color= keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model_color.compile(loss='categorical_crossentropy', optimizer=opt_color, metrics=['accuracy'])\n",
        "cb = callbacks.ModelCheckpoint('model_colorsnd.h5', save_best_only=True, period=1)\n",
        "#model.load_weights(\"baseline_cnn.h5\")\n",
        "\n",
        "hist_color = model_color.fit(x_train, y_train_color, validation_data=(x_validation, y_validation_color), epochs=100, batch_size = 32, callbacks=[cb])\n",
        "loss_and_metrics_color = model_color.evaluate(x_validation, y_validation_color, batch_size=32)\n",
        "print('loss =', loss_and_metrics_color[0],'accuracy =', loss_and_metrics_color[1]);\n",
        "\n",
        "model_color.summary();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDwaxosb9cU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hist_color.history['acc'])\n",
        "plt.plot(hist_color.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['color train', 'color validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(hist_color.history['loss'])\n",
        "plt.plot(hist_color.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['color train', 'color validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "del y_train_color\n",
        "del y_validation_color"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dWJJEf5uw-K",
        "colab_type": "text"
      },
      "source": [
        "rÃ©sults :\n",
        "\n",
        "* C2D(32)+2xC2D(64)+2xC2D(128)+2xC2D(264)+dense(1024)+Dropout(0.5)\n",
        "* epoch = 100, batch size = 32, opt adam\n",
        "* 154ms/step - loss: 1.2560e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Soxr4gUPc2d1",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr66S3C-cwY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_accuracy(y_tab, y_guessed):\n",
        "  accuracy_sum = 0\n",
        "  for i in range(len(y_tab)) :\n",
        "    if(y_tab[i] == y_guessed[i]):\n",
        "      accuracy_sum += 1\n",
        "  accuracy = float(accuracy_sum) / float(len(y_guessed))\n",
        "  print('accuracy = ', accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djLim5d7Ssac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snd_test = glob.glob(\"./audiosynthesis_dl/data/bdd_snd/snd_test/*.wav\") #test_files\n",
        "NB_TEST = len(snd_test)\n",
        "\n",
        "print('[INFO] generation of test mel spectrograms...')\n",
        "x_test = generate_mels(snd_test)\n",
        "\n",
        "sndPath_test = os.listdir( \"./audiosynthesis_dl/data/bdd_snd/snd_test\")[:]\n",
        "label_test = create_label_tab(sndPath_test)\n",
        "print(label_test)\n",
        "\n",
        "print('[INFO] convertion of test labels into int...')\n",
        "y_teststat = label2int(label_test)\n",
        "print(y_teststat)\n",
        "\n",
        "y_teststat_pitch = np.empty(NB_TEST)\n",
        "gen_features_tabs(y_teststat, y_teststat_pitch, 1)\n",
        "print(y_teststat_pitch)\n",
        "\n",
        "y_teststat_thick = np.empty(NB_TEST)\n",
        "gen_features_tabs(y_teststat, y_teststat_thick, 2)\n",
        "\n",
        "y_teststat_color = np.empty(NB_TEST)\n",
        "gen_features_tabs(y_teststat, y_teststat_color, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1avAnBbfdXug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"[INFO] loading networks...\")\n",
        "model_pitch = load_model('model_pitchsnd.h5')\n",
        "model_thick = load_model('model_thicksnd.h5')\n",
        "model_color = load_model('model_colorsnd.h5')\n",
        "\n",
        "y_test_pitch = model_pitch.predict_classes(x_test)\n",
        "y_test_thick = model_thick.predict_classes(x_test)\n",
        "y_test_color = model_color.predict_classes(x_test)\n",
        "\n",
        "# show the inputs and predicted outputs\n",
        "for i in range(len(y_test_pitch)):\n",
        "  print(\"label y_test_pitch[%s] = %s, label y_teststat_pitch[%s] = %s \" % (i, y_test_pitch[i], i, y_teststat_pitch[i]))\n",
        "  print(\"label y_test_thick[%s] = %s, label y_teststat_thick[%s] = %s \" % (i, y_test_thick[i], i, y_teststat_thick[i]))\n",
        "  print(\"label y_test_color[%s] = %s, label y_teststat_color[%s] = %s \\n\" % (i, y_test_color[i], i, y_teststat_color[i]))\n",
        "  \n",
        "print('pitch :')\n",
        "test_accuracy(y_teststat_pitch, y_test_pitch)\n",
        "print('thick :')\n",
        "test_accuracy(y_teststat_thick, y_test_thick)\n",
        "print('color :')\n",
        "test_accuracy(y_teststat_color, y_test_color)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}