---


---

<h1 id="references">REFERENCES</h1>
<p>References read and used in the context of End of Studies internship about Sound synthesis and Sound modification using Deep Learning.</p>
<h2 id="research-papers-">Research papers :</h2>
<ul>
<li>
<p><strong>WaveNet : A Generative Model For Raw Audio</strong><a href="https://arxiv.org/pdf/1609.03499.pdf">https://arxiv.org/pdf/1609.03499.pdf</a></p>
</li>
<li>
<p><strong>Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders :</strong><br>
<a href="https://arxiv.org/pdf/1704.01279.pdf">https://arxiv.org/pdf/1704.01279.pdf</a> – en lecture<br>
<a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a></p>
</li>
<li>
<p><strong>Neural Music Synthesis For Flexible Timbre Control</strong><br>
<a href="https://arxiv.org/pdf/1811.00223.pdf">https://arxiv.org/pdf/1811.00223.pdf</a> – à relire</p>
</li>
<li>
<p><strong>SampleRNN: An Unconditional End-To-End Neural Audio Generation Model</strong><br>
<a href="https://arxiv.org/pdf/1612.07837.pdf">https://arxiv.org/pdf/1612.07837.pdf</a> – à lire</p>
</li>
<li>
<p><strong>Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model</strong><br>
<a href="https://arxiv.org/pdf/1703.10135.pdf">https://arxiv.org/pdf/1703.10135.pdf</a> – en lecture</p>
</li>
<li>
<p><strong>Describing Multimedia Content using Attention-based Encoder–Decoder Networks</strong><br>
<a href="https://arxiv.org/pdf/1507.01053.pdf">https://arxiv.org/pdf/1507.01053.pdf</a> – en lecture</p>
</li>
<li>
<p><strong>Natural TTS Synthesis By Conditioning WaveNet on Mel Spectrogram Predictions</strong><br>
<a href="https://arxiv.org/pdf/1712.05884.pdf">https://arxiv.org/pdf/1712.05884.pdf</a> – a lire</p>
</li>
</ul>
<p>For a better understanding of Neural Networks :</p>
<ul>
<li>
<p><strong>Deep Residual Learning for Image Recognition</strong><br>
<a href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a></p>
</li>
<li>
<p><strong>ImageNet Classification with Deep ConvolutionalNeural Networks</strong><br>
<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
</li>
</ul>
<h2 id="reports-">Reports :</h2>
<ul>
<li><strong>Nicolas Etcheverry’s internship</strong><br>
<a href="http://dept-info.labri.fr/~hanna/Stages/rapport-de-stage.pdf">http://dept-info.labri.fr/~hanna/Stages/rapport-de-stage.pdf</a></li>
</ul>
<h2 id="code-">Code :</h2>
<ul>
<li>
<p><strong>Github WaveNet</strong><br>
<a href="https://github.com/ibab/tensorflow-wavenet">https://github.com/ibab/tensorflow-wavenet</a></p>
</li>
<li>
<p><strong>Nicolas Etcheverry’s project</strong><br>
<a href="https://github.com/etcheverry/transferotron">https://github.com/etcheverry/transferotron</a></p>
</li>
</ul>
<h2 id="definition-articles-">Definition articles :</h2>
<ul>
<li>
<p><strong>ANN</strong><br>
<a href="https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/">https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</a></p>
</li>
<li>
<p><strong>CNN</strong><br>
<a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p>
</li>
<li>
<p><strong>Word2Vec</strong><br>
<a href="https://medium.com/explore-artificial-intelligence/word2vec-a-baby-step-in-deep-learning-but-a-giant-leap-towards-natural-language-processing-40fe4e8602ba">https://medium.com/explore-artificial-intelligence/word2vec-a-baby-step-in-deep-learning-but-a-giant-leap-towards-natural-language-processing-40fe4e8602ba</a></p>
</li>
<li>
<p><strong>RNN</strong><br>
<a href="https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912">https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912</a></p>
</li>
<li>
<p><strong>GRU</strong><br>
<a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be">https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be</a></p>
</li>
<li>
<p><strong>LSTM</strong><br>
<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
</li>
<li>
<p><strong>receptive field</strong><br>
<a href="https://www.quora.com/What-is-a-receptive-field-in-a-convolutional-neural-network">https://www.quora.com/What-is-a-receptive-field-in-a-convolutional-neural-network</a></p>
</li>
<li>
<p><strong>ResNet</strong><br>
<a href="https://towardsdatascience.com/understanding-residual-networks-9add4b664b03">https://towardsdatascience.com/understanding-residual-networks-9add4b664b03</a></p>
</li>
<li>
<p><strong>Bias&amp;Variance</strong><br>
<a href="https://medium.com/datadriveninvestor/bias-and-variance-in-machine-learning-51fdd38d1f86">https://medium.com/datadriveninvestor/bias-and-variance-in-machine-learning-51fdd38d1f86</a></p>
</li>
<li>
<p><strong>One Hot Encoding</strong><br>
<a href="https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f"> https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f</a></p>
</li>
<li>
<p><strong>Embedded Vectors</strong></p>
</li>
<li>
<p><a href="https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12">https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12</a></p>
</li>
</ul>
<p><strong>Attention mechanism</strong><br>
<a href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb">https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb</a></p>
<p><strong>Latent Variable</strong><br>
<a href="https://www.theanalysisfactor.com/what-is-a-latent-variable/">https://www.theanalysisfactor.com/what-is-a-latent-variable/</a></p>
<h2 id="labs-projects--culture">Labs, Projects &amp; Culture</h2>
<ul>
<li>
<p><strong>Magenta</strong><br>
<a href="https://magenta.tensorflow.org/">https://magenta.tensorflow.org/</a><br>
<a href="https://ai.google/research/teams/brain/magenta/">https://ai.google/research/teams/brain/magenta/</a></p>
</li>
<li>
<p><strong>Metacreation Lab</strong><br>
<a href="http://metacreation.net/">http://metacreation.net/</a></p>
</li>
<li>
<p><strong>MatraLab</strong><br>
<a href="http://matralab.hexagram.ca/">http://matralab.hexagram.ca/</a></p>
</li>
</ul>

